{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Steps we followed during project development\n",
    "<img src=\"./workflow.png\">\n",
    "\n",
    "#### Preliminary Phases\n",
    "1. **Exploratory Data Analysis**\n",
    "    In the notebook __preliminary_data_exploration__ we talk about all things we have done looking at the original experiment:\n",
    "    - We perform there an exploratory data analysis, and we look for NaN values in this dataset ( -> no NaN found)\n",
    "   - We put the focus on strange behaviours we found analyzing raw data\n",
    "2. **Dataset Creation with Feature Extraction**\n",
    "    In the notebook __preprocessing__ we talk about all things we need to create our final dataset.\n",
    "   - We analyze raw data to think about what parameters extract from frequency domain, and we discuss the possibility of having sensors with a too small number of frequency peaks\n",
    "    - We discuss there about the window size we want to choose for segmenting raw data\n",
    "      - We perform then a data cleaning step, reasoning about the smoothing we want to adopt\n",
    "   - We do a preliminary dimensionality reduction, searching for too much correlated features\n",
    "\n",
    "#### Core Phases\n",
    "\n",
    "1. **Custom Cross Validation**\n",
    "    In the notebook __cross_validation__ we explain why we decide to have a cross validation that is different from the ones available in the literature.\n",
    "    - We then try to classify with the KFold()\n",
    "    - We implement a cross validation representing a leave-one out strategy in which the \"one\" is an entire subject.\n",
    "    - We compare result, and we see that the first way is a too much optimistic way.\n",
    "2. **Feature Selection**\n",
    "    At the beginning of __classifier_comparison__ notebook, you will find how we decide to keep a number of features in the range [35, 45].\n",
    "3. **Classifiers Comparison**\n",
    "    In the consequence of __classifier_comparison__ we discuss about testing all our pool of classifiers and evaluating them:\n",
    "   - Firstly we compute some metrics (accuracy, f1_score, precision, recall) on six classifiers\n",
    "   - We then decide to analyze better the two that gives us better \"first results\"\n",
    "   - We perform a process of parameter tuning on them and we choose the one that we state as the best.\n",
    "4. **The final Test**\n",
    "    In the notebook __phone_readings_classification__ we try to sse how much good is our classifier with our phone data. The original experiment was driven by an iPhone6, so we want to see if using a different phone, and in particular a different OS, gives us the same results.\n",
    "   - We try to simply classify and roughly comparing the accuracy of the two cases (for speeding up the computation, we use the classic train_test split and not the custom cross validation).\n",
    "   - We see that the accuracy is really poor, but that this could derive from different scaling sensors adopt in our phone\n",
    "   - We try to perform some scaling, and we discuss the new results\n",
    "\n",
    "\n",
    "_PS: You will find all code that generates our processed datasets inside the **dataset_creation.ipynb**. We suggest you to don't run the code, because it takes a really long time, but instead trust the already computed and stored datasets inside the folder \"Processed Datasets\"_"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
